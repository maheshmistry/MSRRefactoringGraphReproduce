{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import datasetconfig\n",
    "from modules import util\n",
    "import re\n",
    "import os.path\n",
    "import hashlib\n",
    "\n",
    "import sys\n",
    "\n",
    "#----------------------------\n",
    "def contains_test_package(path):\n",
    "  path = path.split(\"#\")[0]\n",
    "  test_package = re.findall(r'(test.+?|test)\\.', path, re.IGNORECASE)\n",
    "  contains_test_package = bool(re.search(r'(test.+?|test)\\.', path, re.IGNORECASE))\n",
    "  return contains_test_package\n",
    "\n",
    "#----------------------------\n",
    "def contains_sample_package(path):\n",
    "  path = path.split(\"#\")[0]\n",
    "  sample_package = re.findall(r'(sample.+?|sample)\\.', path, re.IGNORECASE)\n",
    "  contains_sample_package = bool(re.search(r'(sample.+?|sample)\\.', path, re.IGNORECASE))\n",
    "  return contains_sample_package\n",
    "\n",
    "#----------------------------\n",
    "def contains_example_package(path):\n",
    "  path = path.split(\"#\")[0]\n",
    "  example_package = re.findall(r'(example.+?|example)\\.', path, re.IGNORECASE)\n",
    "  contains_example_package = bool(re.search(r'(example.+?|example)\\.', path, re.IGNORECASE))\n",
    "  return contains_example_package\n",
    "\n",
    "#----------------------------\n",
    "def entity_contains_package(name_package, entity):\n",
    "  return bool(re.search(r'(^' + name_package + '.|\\.' + name_package + '\\.|\\.' + name_package + '#)', entity, re.IGNORECASE))\n",
    "\n",
    "#----------------------------\n",
    "def get_duplicated_edges(dataset):\n",
    "  list_occurrence_edges = {}\n",
    "  list_duplicated_edges = []\n",
    "  separator = '||'\n",
    "  for index, row in dataset.iterrows():\n",
    "    entity_before = row.get('entity_before_full_name').replace(\" \", \"\")\n",
    "    entity_after = row.get('entity_after_full_name').replace(\" \", \"\")   \n",
    "    sha1 = row.get('sha1')\n",
    "    key_edge = entity_before + separator + entity_after\n",
    "    #new edge\n",
    "    if key_edge not in list_occurrence_edges:\n",
    "      list_occurrence_edges[key_edge] = 0\n",
    "    list_occurrence_edges[key_edge] += 1 #new ID\n",
    "  for key in list_occurrence_edges:\n",
    "    #is duplicated edge\n",
    "    if (list_occurrence_edges[key] > 1):\n",
    "      list_duplicated_edges.append(key)\n",
    "  return list_duplicated_edges\n",
    "\n",
    "#----------------------------\n",
    "def equals_entities(row):\n",
    "  entity_before = row.get('entity_before_full_name').replace(\" \", \"\")\n",
    "  entity_after = row.get('entity_after_full_name').replace(\" \", \"\")   \n",
    "  refactoring_level = row.get('refactoring_level')\n",
    "  refactoring_name = row.get('refactoring_name')\n",
    "  sha1 = row.get('sha1')\n",
    "  is_equals_entities = (entity_before == entity_after)\n",
    "  return is_equals_entities\n",
    "\n",
    "#----------------------------\n",
    "def contains_constructor(row):\n",
    "  entity_before = row.get('entity_before_full_name').replace(\" \", \"\")\n",
    "  entity_after = row.get('entity_after_full_name').replace(\" \", \"\")  \n",
    "  sha1 = row.get('sha1')\n",
    "  edge_contains_constructor = (\"#new(\" in entity_before) or (\"#new(\" in entity_after)\n",
    "  return edge_contains_constructor\n",
    "\n",
    "#----------------------------\n",
    "def contains_duplicated_edges(row, list_duplicated_edges):\n",
    "  entity_before = row.get('entity_before_full_name').replace(\" \", \"\")\n",
    "  entity_after = row.get('entity_after_full_name').replace(\" \", \"\")\n",
    "  sha1 = row.get('sha1')  \n",
    "  refactoring = row.get('refactoring_name')\n",
    "  key_edge_1 = entity_before + \"||\" + entity_after\n",
    "  key_edge_2 = entity_after + \"||\" + entity_before\n",
    "  #Desconecta os dois v√©rtices completamente, para evitar fluxos sem sentido (ida e volta da aresta)\n",
    "  is_duplicated = (key_edge_1 in list_duplicated_edges) or (key_edge_2 in list_duplicated_edges)\n",
    "  return is_duplicated\n",
    "\n",
    "#----------------------------\n",
    "def entity_contains_exports_keyword(entity):\n",
    "  return bool(re.search(r'(^exports.|^exports#|\\.exports\\.|\\.exports#|#exports$)', entity, re.IGNORECASE))\n",
    "\n",
    "#----------------------------\n",
    "def contains_exports_keyword(row):\n",
    "  entity_before = row.get('entity_before_full_name').replace(\" \", \"\")\n",
    "  entity_after = row.get('entity_after_full_name').replace(\" \", \"\")\n",
    "  return entity_contains_exports_keyword(entity_before) or entity_contains_exports_keyword(entity_after)\n",
    "\n",
    "#----------------------------\n",
    "def is_valid_package(path_before, path_after):\n",
    "  test_package = contains_test_package(path_before) or contains_test_package(path_after)\n",
    "  sample_package = contains_sample_package(path_before) or contains_sample_package(path_after)\n",
    "  example_package = contains_example_package(path_before) or contains_example_package(path_after)\n",
    "  return ((not test_package) and (not sample_package) and (not example_package))\n",
    "\n",
    "#----------------------------\n",
    "def is_valid_refactoring(refactoring, list_duplicated_edges):\n",
    "  is_constructor = contains_constructor(refactoring)  \n",
    "  is_equals_entities = equals_entities(refactoring)\n",
    "  is_duplicated_edges = contains_duplicated_edges(refactoring, list_duplicated_edges)\n",
    "  return ((not is_constructor) and (not is_equals_entities) and (not is_duplicated_edges))\n",
    "\n",
    "#----------------------------\n",
    "def is_core_element(list_duplicated_edges, language, name_project, refactoring):\n",
    "  path_before = refactoring.get('entity_before_full_name')\n",
    "  path_after = refactoring.get('entity_after_full_name')\n",
    "  selected_refactoring_level =  datasetconfig.get_refactoring_level(language)\n",
    "  is_selected_level = (refactoring.get('refactoring_level') == datasetconfig.get_refactoring_level(language))\n",
    "  is_selected_refactoring_type = (refactoring.get('refactoring_name') in datasetconfig.get_dictionary())\n",
    "  return (is_valid_package(path_before, path_after) and (is_selected_level) and (is_selected_refactoring_type) and (is_valid_refactoring(refactoring, list_duplicated_edges)))\n",
    "\n",
    "#----------------------------\n",
    "def get_key_commit(name_project, sha1):\n",
    "  return util.get_name_project_formated(name_project) + \"_\" + str(sha1)\n",
    "\n",
    "#----------------------------\n",
    "def write_core_refactorings_to_csv(file_name, refactorings):\n",
    "  name_output_file =  file_name.replace('.csv', '_selected_operations.csv')\n",
    "  if os.path.isfile(name_output_file):\n",
    "    print('ERROR: File %s exist' % name_output_file)\n",
    "    return\n",
    "  file = open(name_output_file,'w+')\n",
    "  commit_fields = datasetconfig.get_commit_fields()\n",
    "  refdiff_fields = datasetconfig.get_refdiff_fields()\n",
    "  head_list = refdiff_fields + commit_fields\n",
    "  head = (\";\".join(head_list)) + '\\n'\n",
    "  file.write(head)\n",
    "  separator = \";\"\n",
    "  for refactoring in refactorings:\n",
    "    line_list = []\n",
    "    for field in head_list:\n",
    "      line_list.append(('' if refactoring.get(field) is None else str(refactoring.get(field))))\n",
    "    line = (\";\".join(line_list)) + '\\n'\n",
    "    file.write(line)\n",
    "  file.close()\n",
    "  print('Creating %s' % name_output_file)\n",
    "  return\n",
    "\n",
    "#----------------------------\n",
    "def add_commit_properties(language, name_project, filtered_refactoring, commits):\n",
    "  refactorings = []\n",
    "  commit_fields = datasetconfig.get_commit_fields()\n",
    "  for refactoring in filtered_refactoring:\n",
    "    commit_key = get_key_commit(name_project, refactoring.get('sha1'))\n",
    "    commit = commits.get(commit_key)\n",
    "    if commit:\n",
    "      for commit_field in commit_fields: #Adding commit properties (author, date, etc)\n",
    "        refactoring[commit_field] = commit[commit_field]\n",
    "      refactorings.append(refactoring)\n",
    "    else:\n",
    "      print('Commit {} not found'.format(commit_key))\n",
    "  return refactorings\n",
    "\n",
    "#----------------------------\n",
    "def filter_refactorings(list_duplicated_edges, language, name_project, refactorings, commits):\n",
    "  filtered_refactoring = []\n",
    "  for refactoring in refactorings:\n",
    "    if (is_core_element(list_duplicated_edges, language, name_project, refactoring)):\n",
    "      filtered_refactoring.append(refactoring)\n",
    "  return filtered_refactoring\n",
    "\n",
    "#----------------------------\n",
    "def extract_refactorings_from_csv(file_name):\n",
    "  dataset = util.read_csv(file_name)\n",
    "  refactorings = []\n",
    "  for index, row in dataset.iterrows():\n",
    "    refactoring = {}\n",
    "    fields = datasetconfig.get_refdiff_fields()\n",
    "    for field in fields:\n",
    "      refactoring[field] = row.get(field)\n",
    "    refactorings.append(refactoring)\n",
    "  return refactorings\n",
    "\n",
    "#----------------------------\n",
    "def extract_commits_from_csv(file_name):\n",
    "  dataset = util.read_csv(file_name)\n",
    "  commits = {}\n",
    "  for index, row in dataset.iterrows():\n",
    "    commit_info = {}\n",
    "    commit_fields = datasetconfig.get_commit_fields()\n",
    "    for commit_field in commit_fields:\n",
    "      commit_info[commit_field] = row.get(commit_field)\n",
    "    \n",
    "    #Change author name and email to MD5.\n",
    "    commit_info['author_name'] = hashlib.md5((commit_info.get('author_name')).encode(\"utf-8\")).hexdigest() \n",
    "    commit_info['author_email'] = hashlib.md5((commit_info.get('author_email')).encode(\"utf-8\")).hexdigest() \n",
    "    key = get_key_commit(row.get('name_project'), row.get('sha1'))\n",
    "    commits[key] = commit_info\n",
    "  return commits\n",
    "\n",
    "#----------------------------\n",
    "def filter_core_elements():\n",
    "\n",
    "  language = 'java'\n",
    "  for project_name in datasetconfig.get_java_projects():\n",
    "    commits_file = '../../dataset/saner-2020/commits/commits_{}.csv'.format(util.get_name_project_formated(project_name))\n",
    "    refactorings_file = '../../dataset/saner-2020/refactorings/refactorings_{}.csv'.format(util.get_name_project_formated(project_name))\n",
    "\n",
    "    #process commits    \n",
    "    commits = extract_commits_from_csv(commits_file)    \n",
    "\n",
    "    #process refactorings\n",
    "    all_refactorings = extract_refactorings_from_csv(refactorings_file)\n",
    "    list_duplicated_edges = get_duplicated_edges(util.read_csv(refactorings_file))\n",
    "    selected_refactorings = filter_refactorings(list_duplicated_edges, language, project_name, all_refactorings, commits)\n",
    "\n",
    "    #join refactorings and commits\n",
    "    refactorings_and_commits = add_commit_properties(language, project_name, selected_refactorings, commits)\n",
    "\n",
    "    #output\n",
    "    write_core_refactorings_to_csv(refactorings_file, refactorings_and_commits)\n",
    "\n",
    "  return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
